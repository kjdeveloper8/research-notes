
# Research papers 
  Collections of research papers
  
|  Paper | access link  |
|---|---|
| *Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision* | [paper](https://arxiv.org/pdf/2305.03047.pdf) |
| *LIMA: Less Is More for Alignment*| [paper](https://arxiv.org/pdf/2305.11206.pdf)|
| *Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training* | [paper](https://arxiv.org/pdf/2305.14342.pdf) |
| *PMC-LLaMA: Further Finetuning LLaMA on Medical Papers* | [paper](https://arxiv.org/pdf/2304.14454.pdf) |
| *SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models* | [paper](https://arxiv.org/pdf/2303.08896.pdf) |
| *TLDR: Twin Learning for Dimensionality Reduction* | [paper](https://arxiv.org/pdf/2110.09455.pdf) |
| *BloombergGPT: A Large Language Model for Finance* | [paper](https://arxiv.org/pdf/2303.17564.pdf) |
| *LLaMA: Open and Efficient Foundation Language Models* | [paper](https://arxiv.org/pdf/2302.13971.pdf) |
| *BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models* | [paper](https://arxiv.org/pdf/2301.12597.pdf) |
| *VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts* | [paper](https://arxiv.org/pdf/2111.02358.pdf) |
| *Fine-Tuning Language Models from Human Preferences* | [paper](https://arxiv.org/pdf/1909.08593.pdf) |
| *Language Is Not All You Need: Aligning Perception with Language Models* | [paper](https://arxiv.org/pdf/2302.14045.pdf) |
| *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding* | [paper](https://arxiv.org/pdf/1810.04805.pdf) |
| *Prefix-Tuning: Optimizing Continuous Prompts for Generation* | [paper](https://arxiv.org/pdf/2101.00190.pdf) |
| *A Watermark for Large Language Models* | [paper](https://arxiv.org/pdf/2301.10226.pdf) |
| *Multimodal Chain-of-Thought Reasoning in Language Models* | [paper](https://arxiv.org/pdf/2302.00923.pdf) |
| *Direct Preference Optimization: Your Language Model is Secretly a Reward Model* | [paper](https://arxiv.org/pdf/2305.18290.pdf) |
| *QLoRA: Efficient Finetuning of Quantized LLMs* | [paper](https://arxiv.org/pdf/2305.14314.pdf) |
| *To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis* | [paper](https://arxiv.org/pdf/2305.13230.pdf) |
| *The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only* | [paper](https://arxiv.org/pdf/2306.01116.pdf) |
| *Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks* | [paper](https://arxiv.org/pdf/2305.14201.pdf) |
| *Gorilla: Large Language Model Connected with Massive APIs* | [paper](https://arxiv.org/pdf/2305.15334.pdf) |
| *Blockwise Parallel Transformer for Long Context Large Models* | [paper](https://arxiv.org/pdf/2305.19370.pdf) |
| *Scaling Speech Technology to 1,000+ Languages* | [paper](https://arxiv.org/pdf/2305.13516.pdf) |
| *Fine-Tuning Language Models with Just Forward Passes* | [paper](https://arxiv.org/pdf/2305.17333.pdf) |
| *CodeTF: One-stop Transformer Library for State-of-the-art Code LLM* | [paper](https://arxiv.org/pdf/2306.00029.pdf) |
| *Simple and Controllable Music Generation* | [paper](https://arxiv.org/pdf/2306.05284.pdf) |
| *FinGPT: Open-Source Financial Large Language Models* | [paper](https://arxiv.org/pdf/2306.06031.pdf) |
